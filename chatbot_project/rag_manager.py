"""
rag_manager.py - RAG (Retrieval-Augmented Generation) ê´€ë¦¬
RAG + Memory í†µí•© ë²„ì „ (ì˜¤ë¥˜ ìˆ˜ì •)
"""
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from typing import List
from datetime import datetime
import os

from config import Config
from prompts import PromptManager


class RAGManager:
    """RAG ê´€ë¦¬ í´ëž˜ìŠ¤ - ë©”ëª¨ë¦¬ í†µí•©"""

    def __init__(self, config: Config = None, prompt_manager: PromptManager = None, llm = None):
        """
        Args:
            config: ì„¤ì • ê°ì²´
            prompt_manager: í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ìž
            llm: LLM ëª¨ë¸ (LLMManager.llm)
        """
        self.config = config or Config()
        self.prompt_manager = prompt_manager or PromptManager()
        self.llm = llm

        print(f"[RAGManager] ìž„ë² ë”© ëª¨ë¸ ë¡œë”©: {self.config.EMBEDDING_MODEL}")

        self.embeddings = self._initialize_embeddings()
        self.vectorstore = self._initialize_vectorstore()

        print(f"[RAGManager] RAG ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_embeddings(self) -> OpenAIEmbeddings:
        """ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        embeddings = OpenAIEmbeddings(
            model=self.config.EMBEDDING_MODEL,
            openai_api_key=self.config.OPENAI_API_KEY
        )
        print(f"[RAGManager] ìž„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        return embeddings

    def _initialize_vectorstore(self) -> Chroma:
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        print(f"[RAGManager] ChromaDB ì´ˆê¸°í™”")

        # í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™” (ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°©ì§€)
        os.environ["ANONYMIZED_TELEMETRY"] = "False"

        try:
            vectorstore = Chroma(
                persist_directory=self.config.CHROMA_PERSIST_DIR,
                embedding_function=self.embeddings,
                collection_name="elderly_knowledge"
            )
            doc_count = vectorstore._collection.count()
            print(f"[RAGManager] ChromaDB ë¡œë“œ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {doc_count})")
        except Exception as e:
            print(f"[RAGManager] ìƒˆ ChromaDB ìƒì„±")
            vectorstore = Chroma(
                persist_directory=self.config.CHROMA_PERSIST_DIR,
                embedding_function=self.embeddings,
                collection_name="elderly_knowledge"
            )

        return vectorstore

    def create_retriever(self):
        """Retriever ìƒì„±"""
        return self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": self.config.RETRIEVER_K}
        )

    def create_rag_chain(self):
        """
        RAG ì²´ì¸ ìƒì„± (ë©”ëª¨ë¦¬ ì—†ìŒ)
        """
        if not self.llm:
            raise ValueError("LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

        retriever = self.create_retriever()

        prompt = ChatPromptTemplate.from_messages([
            ("system", self.prompt_manager.get_prompt()),
            ("system", "ë‹¤ìŒì€ ê´€ë ¨ ì •ë³´ìž…ë‹ˆë‹¤:\n{context}"),
            ("human", "{question}")
        ])

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        rag_chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return rag_chain

    def create_rag_chain_with_memory(self):
        """
        â­ RAG + ë©”ëª¨ë¦¬ í†µí•© ì²´ì¸ ìƒì„±
        """
        if not self.llm:
            raise ValueError("LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

        retriever = self.create_retriever()

        # â­ ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•˜ëŠ” í”„ë¡¬í”„íŠ¸
        prompt = ChatPromptTemplate.from_messages([
            ("system", self.prompt_manager.get_prompt()),
            ("system", "ë‹¤ìŒì€ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ìž…ë‹ˆë‹¤:\n{context}"),
            MessagesPlaceholder(variable_name="chat_history"),  # â† ëŒ€í™” ê¸°ë¡
            ("human", "{question}")
        ])

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        # â­ ìˆ˜ì •: RunnablePassthrough ëŒ€ì‹  ëª…ì‹œì  í•¨ìˆ˜ ì‚¬ìš©
        def get_question(x):
            if isinstance(x, dict):
                return x.get("question", "")
            return x

        def get_history(x):
            if isinstance(x, dict):
                return x.get("chat_history", [])
            return []

        rag_chain = (
            {
                "context": lambda x: format_docs(retriever.invoke(get_question(x))),
                "question": get_question,
                "chat_history": get_history
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return rag_chain

    def generate_with_rag(self, query: str) -> tuple[str, List[Document]]:
        """
        RAGë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± (ë©”ëª¨ë¦¬ ì—†ìŒ)

        Args:
            query: ì‚¬ìš©ìž ì§ˆë¬¸

        Returns:
            tuple: (ì‘ë‹µ, ì¶œì²˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸)
        """
        rag_chain = self.create_rag_chain()
        retriever = self.create_retriever()

        # ì‘ë‹µ ìƒì„±
        response = rag_chain.invoke(query)

        # ì¶œì²˜ ë¬¸ì„œ ê²€ìƒ‰
        source_docs = retriever.invoke(query)

        return response, source_docs

    def generate_with_rag_and_memory(
        self,
        query: str,
        chat_history: List
    ) -> tuple[str, List[Document]]:
        """
        â­ RAG + ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±

        Args:
            query: ì‚¬ìš©ìž ì§ˆë¬¸
            chat_history: ëŒ€í™” ê¸°ë¡ (LangChain Message í˜•ì‹)

        Returns:
            tuple: (ì‘ë‹µ, ì¶œì²˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸)
        """
        print(f"[RAGManager] RAG + ë©”ëª¨ë¦¬ ëª¨ë“œ: {len(chat_history)}ê°œ ëŒ€í™” ê¸°ë¡ ì‚¬ìš©")

        rag_chain = self.create_rag_chain_with_memory()
        retriever = self.create_retriever()

        # ì‘ë‹µ ìƒì„± (ëŒ€í™” ê¸°ë¡ í¬í•¨)
        response = rag_chain.invoke({
            "question": query,
            "chat_history": chat_history
        })

        # ðŸ” ë””ë²„ê·¸: ì‘ë‹µ í™•ì¸
        print(f"[RAGManager DEBUG] ì‘ë‹µ íƒ€ìž…: {type(response)}")
        print(f"[RAGManager DEBUG] ì‘ë‹µ ë‚´ìš©: '{response}'")
        print(f"[RAGManager DEBUG] ì‘ë‹µ ê¸¸ì´: {len(str(response))}")

        # ì¶œì²˜ ë¬¸ì„œ ê²€ìƒ‰
        source_docs = retriever.invoke(query)

        return response, source_docs

    def add_document(self, content: str, metadata: dict = None) -> dict:
        """
        ë¬¸ì„œ ì¶”ê°€

        Args:
            content: ë¬¸ì„œ ë‚´ìš©
            metadata: ë©”íƒ€ë°ì´í„°

        Returns:
            dict: ê²°ê³¼ ì •ë³´
        """
        try:
            # tiktoken ì—†ì´ ìž‘ë™í•˜ë„ë¡ ê°„ë‹¨í•œ ë¶„í•  ì‚¬ìš©
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.config.CHUNK_SIZE,
                chunk_overlap=self.config.CHUNK_OVERLAP,
                length_function=len,
                is_separator_regex=False,  # tiktoken ì‚¬ìš© ì•ˆ í•¨
                separators=["\n\n", "\n", ". ", " ", ""]
            )

            chunks = text_splitter.split_text(content)

            documents = [
                Document(
                    page_content=chunk,
                    metadata=metadata or {
                        "source": "manual",
                        "timestamp": str(datetime.now())
                    }
                )
                for chunk in chunks
            ]

            self.vectorstore.add_documents(documents)

            print(f"[RAGManager] ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ ({len(chunks)}ê°œ ì²­í¬)")

            return {
                "success": True,
                "chunks_created": len(chunks)
            }

        except Exception as e:
            print(f"[RAGManager] ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    def search_documents(self, query: str, k: int = None) -> List[Document]:
        """
        ë¬¸ì„œ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜

        Returns:
            List[Document]: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        k = k or self.config.RETRIEVER_K
        docs = self.vectorstore.similarity_search(query, k=k)
        return docs

    def get_document_count(self) -> int:
        """ë²¡í„° DBì˜ ë¬¸ì„œ ìˆ˜ ì¡°íšŒ"""
        return self.vectorstore._collection.count()

    def clear_documents(self) -> bool:
        """ë²¡í„° DB ì´ˆê¸°í™”"""
        try:
            import shutil

            if os.path.exists(self.config.CHROMA_PERSIST_DIR):
                shutil.rmtree(self.config.CHROMA_PERSIST_DIR)

            self.vectorstore = self._initialize_vectorstore()

            print(f"[RAGManager] ë²¡í„° DB ì´ˆê¸°í™” ì™„ë£Œ")
            return True

        except Exception as e:
            print(f"[RAGManager] ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False