#!/usr/bin/env python3
"""Plot the live microphone signal(s) with matplotlib.

Matplotlib and NumPy have to be installed.

"""
import argparse
from dataclasses import dataclass
import queue
import sys
from openai import OpenAI
import time
from silero_vad import load_silero_vad, read_audio, get_speech_timestamps
import soundfile as sf

from matplotlib.animation import FuncAnimation
import matplotlib.pyplot as plt
import numpy as np
import sounddevice as sd

client = OpenAI(api_key="")

@dataclass
class audioArgs:
    device: int = None
    samplerate: int = 16000 # 
    channels: int = 1  # 1 for 'mono' or 2 for 'stereo'
    chunksize: int = 64
    batch_size : int = 100
    audiomin_threshold : float = 0.4  # ì—°ì† ë¬´ìŒ ê°ì§€ ì„ê³„ê°’

class AudioStream:
    def __init__(self):
        self.Queue = queue.Queue()
        self.stream = None

    def init_stream(self):
        if self.stream is None:
            self.stream = sd.InputStream(
                device= audioArgs().device,
                blocksize=audioArgs.chunksize,
                channels=audioArgs.channels,
                samplerate=audioArgs.samplerate, 
                callback=self.audio_callback)
            print("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            print("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    def start_stream(self):
        if self.stream is not None:
            self.stream.start()
            print("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨")
        else:
            print("ìŠ¤íŠ¸ë¦¼ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € init_stream()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

    def stop_stream(self):
        if self.stream is not None:
            self.stream.stop()
            self.stream.close()
            self.stream = None
            print("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œë¨")
        else:
            print("ì¢…ë£Œí•  ìŠ¤íŠ¸ë¦¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    def process_audio_batch(self,target = audioArgs.batch_size):
        chunks = []
        
        while len(chunks) < target:
            # íì—ì„œ í•˜ë‚˜ì”© ê°€ì ¸ì™€ì„œ ë²„í¼ì— ì¶”ê°€
            chunk = self.Queue.get(timeout=1.0)
            chunks.append(chunk)
            
        if chunks:
            return np.concatenate(chunks, axis=0).squeeze()
        else:
            return None     
        
    def audio_callback(self,indata, frames, time, status):
        """This is called (from a separate thread) for each audio block."""
        if status:
            print(status, file=sys.stderr)
        # Fancy indexing with mapping creates a (necessary!) copy:
        self.Queue.put(indata.copy())



class AudioActivityDetection:
    def __init__(self, silence_threshold: int = 3):
        self.is_recording = False
        self.speech_buffer = []
        self.speech_id = 0
        self.prev_status = "nonspeech"
        self.stop_count = 0
        self.silence_threshold = silence_threshold

    def __call__(self, time_checker, buffers):
        self.current_status = len(time_checker) > 0
        
        if self.current_status:  # ìŒì„± ê°ì§€ë¨
            if not self.is_recording:
                self.is_recording = True
                self.speech_buffer = []
                print("ğŸ¤ ìŒì„± ì‹œì‘")
            
            self.speech_buffer.append(buffers)
            
            # âœ… ìŒì„±ì´ ê°ì§€ë˜ë©´ ë¬´ìŒ ì¹´ìš´íŠ¸ ë¦¬ì…‹!
            if self.stop_count > 0:
                print(f"ìŒì„± ì¬ê°ì§€ â†’ ë¬´ìŒ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ({self.stop_count} â†’ 0)")
                self.stop_count = 0
            
            self.prev_status = "speech"
            
        else:  # ë¬´ìŒ
            if self.is_recording:
                zero_data = np.zeros_like(buffers)
                self.speech_buffer.append(zero_data)
                
                # âœ… ì—°ì† ë¬´ìŒë§Œ ì¹´ìš´íŠ¸
                self.stop_count += 1
                self.prev_status = "nonspeech"
                
                print(f"ì—°ì† ë¬´ìŒ: {self.stop_count}/{self.silence_threshold}")
                
                if self.stop_count >= self.silence_threshold:
                    speech_data = np.concatenate(self.speech_buffer, axis=0)
                    self.is_recording = False
                    self.stop_count = 0
                    self.speech_buffer = []
                    
                    print(f"ğŸ›‘ ì—°ì† {self.silence_threshold}ë²ˆ ë¬´ìŒìœ¼ë¡œ ì¢…ë£Œ")
                    return speech_data

        return None

def listen_and_transcribe():
    """ìŒì„±ì„ ìˆ˜ì§‘í•˜ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    model = load_silero_vad()
    
    stream = AudioStream()
    stream.init_stream()
    stream.start_stream()
    event_checker = AudioActivityDetection()

    print("ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨ - ë§ì”€í•´ì£¼ì„¸ìš”")

    while True:
        audio_data = stream.process_audio_batch(target=audioArgs.batch_size)
        if audio_data is not None:
            print(f"ë°°ì¹˜ í¬ê¸°: {audio_data.shape}")
            
            speech_timestamps = get_speech_timestamps(
                audio_data,
                model,
                return_seconds=False,
            )
            print(f"ìŒì„± êµ¬ê°„: {speech_timestamps}")
            result = event_checker(
                speech_timestamps,
                audio_data
            )
            if result is not None:
                print(f"ì €ì¥ëœ ìŒì„± í´ë¦½ {event_checker.speech_id}, ê¸¸ì´: {result.shape}")
                sf.write("temp_audio.wav", result, samplerate=audioArgs.samplerate)

                with open("temp_audio.wav", "rb") as audio_file:
                    response = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_file,
                        language="ko"
                    )

                transcript_text = response.text
                print(f"ë³€í™˜ëœ í…ìŠ¤íŠ¸: {transcript_text}")
                
                # ìŠ¤íŠ¸ë¦¼ ë©ˆì¶”ê³  í…ìŠ¤íŠ¸ ë°˜í™˜
                stream.stop_stream()
                return transcript_text

        else:
            print("ë°°ì¹˜ ìˆ˜ì§‘ ì‹¤íŒ¨")
            time.sleep(0.1)

if __name__ == '__main__':
    text = listen_and_transcribe()
    print(f"\nìµœì¢… ê²°ê³¼: {text}")