#!/usr/bin/env python3
"""ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ"""
import dataclasses
import queue
import sys
from librosa import stream
from openai import OpenAI
import time
from silero_vad import load_silero_vad, get_speech_timestamps
import soundfile as sf
import numpy as np
import sounddevice as sd
import os
from dotenv import load_dotenv

load_dotenv()  # ì´ ì¤„ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨


# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY", "your-api-key")
client = OpenAI(api_key=api_key)

@dataclasses.dataclass
class AudioConfig:
    """ì˜¤ë””ì˜¤ ì„¤ì • ìƒìˆ˜"""
    SAMPLERATE = 16000
    SILENCE_THRESHOLD = 3
    EXIT_THRESHOLD = 10


class VADModel:
    """
    ìŒì„±ì„ ê°ì§€í•˜ëŠ” VAD ëª¨ë¸ ë˜í¼ í´ë˜ìŠ¤ (private)
    ìƒì„±í•˜ë©´ ë™ì‹œì— VAD ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Attributes:
        model: ë¡œë“œëœ VAD ëª¨ë¸
    """
    def __init__(self,monitoring = False)-> None:
        self.model = load_silero_vad()
        self.SAMPLERATE = AudioConfig.SAMPLERATE
        self.monitoring = monitoring

    """
    ì˜¤ë””ì˜¤ ë°ì´í„°ì—ì„œ ìŒì„± êµ¬ê°„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        audio_data (np.array): ì˜¤ë””ì˜¤ ì‹ í˜¸ ë°°ì—´

    Returns:
        list: ê°ì§€ëœ ìŒì„± êµ¬ê°„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸
    """    
    def get_speech_timestamps(self, audio_data)->list:
        if self.monitoring:
            print(f"[VAD] audio_data type: {type(audio_data)}")
            print(f"[VAD] audio_data dtype: {audio_data.dtype}")
            print(f"[VAD] audio_data shape: {audio_data.shape}")
            print(f"[VAD] audio_data range: [{audio_data.min():.4f}, {audio_data.max():.4f}]")
  
        return get_speech_timestamps(
            audio_data,
            self.model,
            sampling_rate = self.SAMPLERATE,
        )



class _AudioActivityDetection:
    """
    ìŒì„± ë°ì´í„°ë¥¼ ì½ì–´ ì™€ì„œ í™”ìê°€ ëŒ€í™”ë¥¼ í•˜ê³  ìˆëŠ”ì§€ ê°ì‹œ
    Status
    - 1. Silent: ë¬´ìŒ ìƒíƒœ
    - 2. Speech: ìŒì„± ê°ì§€ ìƒíƒœ
    - 3. Finished: ìŒì„± ë…¹ìŒ ì¢…ë£Œ ìƒíƒœ
    - 4. Error: ì—°ì† ë¬´ìŒìœ¼ë¡œ ì¸í•œ ì‹œìŠ¤í…œ ì¢…ë£Œ ìƒíƒœ
    - 5. Reset: ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì´ˆê¸°í™” ìƒíƒœ
    
    Attributes:
        is_recording:  í˜„ì¬ ë…¹ìŒì¤‘ì¸ ì—¬ë¶€ë¡œ ìµœì´ˆë¡œ ìŒì„±ì´ ê°ì§€ë˜ë©´ Trueë¡œ ë³€ê²½ë˜ê³ ,
                       ì—°ì†ìœ¼ë¡œ ë¬´ìŒì´ silence_thresholdë²ˆ ê°ì§€ë˜ë©´ Falseë¡œ ë³€ê²½ë©ë‹ˆë‹¤.
        speech_buffer: ë…¹ìŒëœ ìŒì„± ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë²„í¼  
        stop_count: ì—°ì† ë¬´ìŒ ì¹´ìš´íŠ¸
        silence_threshold: ì—°ì† ë¬´ìŒìœ¼ë¡œ ê°„ì£¼í•˜ëŠ” ì„ê³„ê°’
        exit_threshold: ì—°ì† ë¬´ìŒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ì‹œìŠ¤í…œ ì¢…ë£Œí•˜ëŠ” ì„ê³„ê°’
    
    Methods:
        resetStream(): ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì´ˆê¸°í™”
        __call__(speech_detected, audio_buffer): ìŒì„± ë°ì´í„°ì—ì„œ í™”ì í™œë™ì„ ê°ì§€í•˜ê³  ë…¹ìŒ ì‹œì‘/ì¢…ë£Œë¥¼ ì œì–´

    """
    def __init__(self, 
                 silence_threshold: int = AudioConfig.SILENCE_THRESHOLD,
                 exit_threshold: int = AudioConfig.EXIT_THRESHOLD):
        self.is_recording = False
        self.speech_buffer = []
        self.stop_count = 0
        self.silence_threshold = silence_threshold
        self.exit_threshold = exit_threshold

    def resetStream(self):
        """ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì´ˆê¸°í™”"""
        self.is_recording = False
        self.speech_buffer = []
        self.stop_count = 0
        return {"audio": None, "status": "Reset"}

    def __call__(self, 
                 speech_detected: list,
                 audio_buffer: np.array) -> dict:
        """
        ìŒì„± ë°ì´í„°ì—ì„œ í™”ì í™œë™ì„ ê°ì§€í•˜ê³  ë…¹ìŒ ì‹œì‘/ì¢…ë£Œë¥¼ ì œì–´í•©ë‹ˆë‹¤.

        Args:
            speech_detected (list): ê°ì§€ëœ ìŒì„± êµ¬ê°„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸
            audio_buffer (np.array): í˜„ì¬ ì˜¤ë””ì˜¤ ë²„í¼ ë°ì´í„°
        Returns:
            np.array or None: ë…¹ìŒì´ ì¢…ë£Œë˜ì—ˆì„ ë•Œ ì™„ì„±ëœ ìŒì„± ë°ì´í„° ë°°ì—´, 
                                ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ None, ìµœì¢… ì›¨ì´ë¸Œ íŒŒì¼ì´ ìƒì„±ë˜ê¸° ì „ê¹Œì§€ ë°˜í™˜ì„ Noneìœ¼ë¡œí•¨         
        """
        has_speech = len(speech_detected) > 0
        user_status = "Silent" #ì—†ìœ¼ë©´ Silent, ê°•ì œ ì¢…ë£Œë˜ë©´ Errorë¡œ ì „ì†¡
        user_audio = None
        
        
        if has_speech:
            if not self.is_recording:
                self.is_recording = True
                self.stop_count = 0
                self.speech_buffer = []
                user_status = "Speech"
                print("ğŸ¤ ìŒì„± ì‹œì‘")
            
            self.speech_buffer.append(audio_buffer)
            
            if self.stop_count > 0:
                print(f"ìŒì„± ì¬ê°ì§€ â†’ ë¬´ìŒ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ({self.stop_count} â†’ 0)")
                self.stop_count = 0
            
        else:  # ë¬´ìŒ
            if self.is_recording:
                zero_data = np.zeros_like(audio_buffer)
                self.speech_buffer.append(zero_data)
                self.stop_count += 1
                user_status = "Speech" #ë¬´ìŒì´ì–´ë„ ë…¹ìŒì¤‘ì´ë‹ˆ Speechë¡œ ì „ì†¡
                
                print(f"ì—°ì† ë¬´ìŒ: {self.stop_count}/{self.silence_threshold}")
                
                if self.stop_count >= self.silence_threshold:
                    speech_data = np.concatenate(self.speech_buffer, axis=0)
                    self.is_recording = False
                    self.stop_count = 0
                    self.speech_buffer = []
                    user_audio = speech_data
                    user_status = "Finished"
                    
            else:
                self.stop_count += 1
                if self.stop_count >= self.exit_threshold:
                    print(f"âŒ ì—°ì† {self.exit_threshold}ë²ˆ ë¬´ìŒìœ¼ë¡œ ì‹œìŠ¤í…œ ì¢…ë£Œ")
                    user_audio = None
                    user_status = "Error"
                else:
                    user_status = "Silent"


        return {"audio": user_audio, "status": user_status}

_event_checker = _AudioActivityDetection()
_vad_model = VADModel(monitoring=False)

def process_audio_chunk(audio_data,
                        reset:bool=False)-> dict:
    """
    ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²­ì·¨ ë° í…ìŠ¤íŠ¸ ë³€í™˜ ë‚´ë¶€ í•¨ìˆ˜
    
    Status
    - 1. Silent: ë¬´ìŒ ìƒíƒœ
    - 2. Speech: ìŒì„± ê°ì§€ ìƒíƒœ
    - 3. Finished: ìŒì„± ë…¹ìŒ ì¢…ë£Œ ìƒíƒœ
    - 4. Error: ì—°ì† ë¬´ìŒìœ¼ë¡œ ì¸í•œ ì‹œìŠ¤í…œ ì¢…ë£Œ ìƒíƒœ
    - 5. Reset: ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì´ˆê¸°í™” ìƒíƒœ
    
    Args:
        audio_data (np.array): ì˜¤ë””ì˜¤ ì‹ í˜¸ ë°°ì—´
        reset (bool): ìŠ¤íŠ¸ë¦¼ ìƒíƒœ ì´ˆê¸°í™” ì—¬ë¶€
    Returns:
        dict: {
            "status": "Silent" | "Speech" | "Finished" | "Error" | "Reset", 
            "text": ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        }
    """
    event_checker = _event_checker  
    vad_model = _vad_model
    
    result_status = None
    transcript_text = None

    if reset:
        result = event_checker.resetStream()
        return {"status": result["status"], "text": None}

    if audio_data is not None:
        speech_timestamps = vad_model.get_speech_timestamps(audio_data)
        result = event_checker(speech_timestamps, audio_data)
                
        if result["status"] == "Finished":
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            sf.write("temp_audio.wav", result["audio"], samplerate=AudioConfig.SAMPLERATE)

            # OpenAI Whisper APIë¡œ ì „ì‚¬
            with open("temp_audio.wav", "rb") as audio_file:
                response = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="ko"
                )

            result_status = result["status"]
            transcript_text = response.text

        elif result["status"] == "Error":
            result_status = result["status"]
            transcript_text = None

        elif result["status"] in ["Speech", "Silent"]:
            result_status = result["status"]
            transcript_text = None

        elif result["status"] == "Reset":
            result_status = result["status"]
            transcript_text = None
                    
    return {"status": result_status, "text": transcript_text}   
                


# ========== PUBLIC API ==========


if __name__ == '__main__':
    # CLI ëª¨ë“œ: ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹
    result = process_audio_chunk(audio_data=None, reset=True)
    print(f"\nìµœì¢… ê²°ê³¼: {result}")