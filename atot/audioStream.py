#!/usr/bin/env python3
"""ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œ"""
import queue
import sys
from openai import OpenAI
import time
from silero_vad import load_silero_vad, get_speech_timestamps
import soundfile as sf
from fastapi import UploadFile, File
import numpy as np
import sounddevice as sd
import os
from dotenv import load_dotenv

load_dotenv()  # ì´ ì¤„ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨


# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY", "your-api-key")
client = OpenAI(api_key=api_key)

# ì „ì—­ VAD ëª¨ë¸ (ì‹±ê¸€í†¤ íŒ¨í„´)
_vad_model = None

class AudioConfig:
    """ì˜¤ë””ì˜¤ ì„¤ì • ìƒìˆ˜"""
    DEVICE = None
    SAMPLERATE = 16000
    CHANNELS = 1
    CHUNKSIZE = 64
    BATCH_SIZE = 100
    SILENCE_THRESHOLD = 3

class _VADModel:
    """
    ìŒì„±ì„ ê°ì§€í•˜ëŠ” VAD ëª¨ë¸ ë˜í¼ í´ë˜ìŠ¤ (private)
    ìƒì„±í•˜ë©´ ë™ì‹œì— VAD ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Attributes:
        model: ë¡œë“œëœ VAD ëª¨ë¸
    """
    def __init__(self)-> None:
        self.model = load_silero_vad()

    """
    ì˜¤ë””ì˜¤ ë°ì´í„°ì—ì„œ ìŒì„± êµ¬ê°„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        audio_data (np.array): ì˜¤ë””ì˜¤ ì‹ í˜¸ ë°°ì—´

    Returns:
        list: ê°ì§€ëœ ìŒì„± êµ¬ê°„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸
    """    
    def get_speech_timestamps(self, audio_data)->list:

        print(f"[VAD] audio_data type: {type(audio_data)}")
        print(f"[VAD] audio_data dtype: {audio_data.dtype}")
        print(f"[VAD] audio_data shape: {audio_data.shape}")
        print(f"[VAD] audio_data range: [{audio_data.min():.4f}, {audio_data.max():.4f}]")
  
        return get_speech_timestamps(
            audio_data,
            self.model,
        )

class _AudioStream:
    """
    ìŒì„±ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ í´ë˜ìŠ¤ (private)
    chunk ë‹¨ìœ„ë¡œ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ íì— ì €ì¥í•©ë‹ˆë‹¤.

    ì¶”í›„ í”„ë¡ íŠ¸ì—”ë“œì™€ í˜‘ì˜í•´ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²­í¬ë¥¼ ë°›ì•„ë“¤ì¸ë‹¤ê³  í•˜ë©´ í•„ìš”ì—†ëŠ” í´ë˜ìŠ¤
    
    Attributes:
        queue:  ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì €ì¥í•˜ëŠ” í
        stream: ì‚¬ìš´ë“œë””ë°”ì´ìŠ¤ ì…ë ¥ ìŠ¤íŠ¸ë¦¼
    """
    def __init__(self)-> None:
        self.queue = queue.Queue()
        self.stream = None


    def init_stream(self):
        """
        input ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™”

        """        
        if self.stream is None:
            self.stream = sd.InputStream(
                device=AudioConfig.DEVICE,
                blocksize=AudioConfig.CHUNKSIZE,
                channels=AudioConfig.CHANNELS,
                samplerate=AudioConfig.SAMPLERATE, 
                callback=self._audio_callback
            )
            print("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™” ì™„ë£Œ")


    def start_stream(self):
        """
        input ìŠ¤íŠ¸ë¦¼ ì‹œì‘ìœ¼ë¡œ ì‚¬ì „ì— ì´ˆê¸°í™” í•„ìš”

        Raises:
            RuntimeError: ìŠ¤íŠ¸ë¦¼ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì„ ë•Œ
        """
        if self.stream is not None:
            self.stream.start()
            print("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨")
        else:
            raise RuntimeError("ìŠ¤íŠ¸ë¦¼ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")



    def stop_stream(self):
        """
        input ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ

        """
        if self.stream is not None:
            self.stream.stop()
            self.stream.close()
            self.stream = None
            print("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œë¨")
        elif self.stream is None:
            print("ìŠ¤íŠ¸ë¦¼ì´ ì´ë¯¸ ì¢…ë£Œë˜ì—ˆê±°ë‚˜ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def process_audio_batch(self, 
                            target=AudioConfig.BATCH_SIZE):
        """
        chunk ë‹¨ìœ„ë¡œ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ë°˜í™˜

        Args:
            target (int): ìˆ˜ì§‘í•  ì²­í¬ì˜ ê°œìˆ˜, ì´ ìƒ˜í”Œì˜ ê°œìˆ˜ëŠ” ì…ë ¥ë‹¨ìœ„ë¡œ ë°›ëŠ” ìƒ˜í”Œìˆ˜ * target ìœ¼ë¡œ ê³„ì‚°
        Returns:
            np.array: ìˆ˜ì§‘ëœ ì˜¤ë””ì˜¤ ì‹ í˜¸ ë°°ì—´

        """    
        chunks = []
        
        try:
            while len(chunks) < target:
                chunk = self.queue.get(timeout=1.0)
                chunks.append(chunk)
        except queue.Empty:
            pass
            
        return np.concatenate(chunks, axis=0).squeeze() if chunks else None
        
    def _audio_callback(self, indata, frames, time, status):
        """
        ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì½œë°± í•¨ìˆ˜, ìŠ¤íŠ¸ë¦¼ í´ë˜ìŠ¤ì˜ íì— ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            indata (np.array): ì…ë ¥ ì˜¤ë””ì˜¤ ë°ì´í„°   
            frames (int): í”„ë ˆì„ ìˆ˜
            time: íƒ€ì„ ì •ë³´
            status: ìƒíƒœ ì •ë³´
        Returns:
            None

        """            
        
        if status:
            print(status, file=sys.stderr)
        self.queue.put(indata.copy())


class _AudioActivityDetection:
    """
    ìŒì„± ë°ì´í„°ë¥¼ ì½ì–´ ì™€ì„œ í™”ìê°€ ëŒ€í™”ë¥¼ í•˜ê³  ìˆëŠ”ì§€ ê°ì‹œ
    
    Attributes:
        is_recording:  í˜„ì¬ ë…¹ìŒì¤‘ì¸ ì—¬ë¶€ë¡œ ìµœì´ˆë¡œ ìŒì„±ì´ ê°ì§€ë˜ë©´ Trueë¡œ ë³€ê²½ë˜ê³ ,
                       ì—°ì†ìœ¼ë¡œ ë¬´ìŒì´ silence_thresholdë²ˆ ê°ì§€ë˜ë©´ Falseë¡œ ë³€ê²½ë©ë‹ˆë‹¤.
        speech_buffer: ë…¹ìŒëœ ìŒì„± ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë²„í¼  
        stop_count: ì—°ì† ë¬´ìŒ ì¹´ìš´íŠ¸
        silence_threshold: ì—°ì† ë¬´ìŒìœ¼ë¡œ ê°„ì£¼í•˜ëŠ” ì„ê³„ê°’
    """
    def __init__(self, silence_threshold: int = AudioConfig.SILENCE_THRESHOLD):
        self.is_recording = False
        self.speech_buffer = []
        self.stop_count = 0
        self.silence_threshold = silence_threshold

    def __call__(self, 
                 speech_detected: list,
                 audio_buffer: np.array) -> np.array:
        """
        ìŒì„± ë°ì´í„°ì—ì„œ í™”ì í™œë™ì„ ê°ì§€í•˜ê³  ë…¹ìŒ ì‹œì‘/ì¢…ë£Œë¥¼ ì œì–´í•©ë‹ˆë‹¤.

        Args:
            speech_detected (list): ê°ì§€ëœ ìŒì„± êµ¬ê°„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸
            audio_buffer (np.array): í˜„ì¬ ì˜¤ë””ì˜¤ ë²„í¼ ë°ì´í„°
        Returns:
            np.array or None: ë…¹ìŒì´ ì¢…ë£Œë˜ì—ˆì„ ë•Œ ì™„ì„±ëœ ìŒì„± ë°ì´í„° ë°°ì—´, 
                                ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ None, ìµœì¢… ì›¨ì´ë¸Œ íŒŒì¼ì´ ìƒì„±ë˜ê¸° ì „ê¹Œì§€ ë°˜í™˜ì„ Noneìœ¼ë¡œí•¨         
        """
        has_speech = len(speech_detected) > 0
        
        if has_speech:
            if not self.is_recording:
                self.is_recording = True
                self.speech_buffer = []
                print("ğŸ¤ ìŒì„± ì‹œì‘")
            
            self.speech_buffer.append(audio_buffer)
            
            if self.stop_count > 0:
                print(f"ìŒì„± ì¬ê°ì§€ â†’ ë¬´ìŒ ì¹´ìš´íŠ¸ ë¦¬ì…‹ ({self.stop_count} â†’ 0)")
                self.stop_count = 0
            
        else:  # ë¬´ìŒ
            if self.is_recording:
                zero_data = np.zeros_like(audio_buffer)
                self.speech_buffer.append(zero_data)
                self.stop_count += 1
                
                print(f"ì—°ì† ë¬´ìŒ: {self.stop_count}/{self.silence_threshold}")
                
                if self.stop_count >= self.silence_threshold:
                    speech_data = np.concatenate(self.speech_buffer, axis=0)
                    self.is_recording = False
                    self.stop_count = 0
                    self.speech_buffer = []
                    self.speech_id += 1
                    
                    print(f"ğŸ›‘ ì—°ì† {self.silence_threshold}ë²ˆ ë¬´ìŒìœ¼ë¡œ ì¢…ë£Œ")
                    return speech_data

        return None


def _get_vad_model():
    """
    ì „ì—­ VAD ëª¨ë¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜

    Returns:
        _VADModel: VAD ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    """
    global _vad_model
    if _vad_model is None:
        _vad_model = _VADModel()
    return _vad_model


def _listen_and_transcribe():
    """ë‚´ë¶€ í•¨ìˆ˜: ì‹¤ì‹œê°„ ìŒì„± ìˆ˜ì§‘ ë° í…ìŠ¤íŠ¸ ë³€í™˜"""
    vad_model = _get_vad_model()
    stream = _AudioStream()
    stream.init_stream()
    stream.start_stream()
    event_checker = _AudioActivityDetection()

    print("ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨ - ë§ì”€í•´ì£¼ì„¸ìš”")


    while True:
            audio_data = stream.process_audio_batch()
            print(f"ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì‹ : {audio_data.shape if audio_data is not None else None}")
            print(audio_data)
            
            if audio_data is not None:
                speech_timestamps = vad_model.get_speech_timestamps(audio_data)
                print(f"ê°ì§€ëœ ìŒì„± êµ¬ê°„: {speech_timestamps}")

                result = event_checker(speech_timestamps, audio_data)
                print("ì´ë²¤íŠ¸ ì²´í¬ ì™„ë£Œ")
                
                if result is not None:
                    print(f"ì €ì¥ëœ ìŒì„± í´ë¦½ {event_checker.speech_id}, ê¸¸ì´: {result.shape}")
                    
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    sf.write("temp_audio.wav", result, samplerate=AudioConfig.SAMPLERATE)

                    # OpenAI Whisper APIë¡œ ì „ì‚¬
                    with open("temp_audio.wav", "rb") as audio_file:
                        response = client.audio.transcriptions.create(
                            model="whisper-1",
                            file=audio_file,
                            language="ko"
                        )

                    transcript_text = response.text
                    print(f"ë³€í™˜ëœ í…ìŠ¤íŠ¸: {transcript_text}")
                    
                    return transcript_text
            else:
                time.sleep(0.1)
                


# ========== PUBLIC API ==========

def audio2text(mode: str = "stream",
               wavefile: UploadFile = File(None)) -> str:
    """
    ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í†µí•© API, í•´ë‹¹ í•¨ìˆ˜ë¥¼ í†µí•´ì„œ ë¡œì»¬ ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ”
    íŒŒì¼ ì—…ë¡œë“œ ë°©ì‹ìœ¼ë¡œ ìŒì„± ì¸ì‹ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    Args:
        mode: "stream" (ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥) ë˜ëŠ” "file" (íŒŒì¼ ì…ë ¥)
        wavefile: modeê°€ "file"ì¼ ë•Œ í•„ìš”í•œ ì˜¤ë””ì˜¤ íŒŒì¼ (UploadFile)
    
    Returns:
        str: ë³€í™˜ëœ í…ìŠ¤íŠ¸
    
    Raises:
        ValueError: modeê°€ "file"ì¸ë° wavefileì´ Noneì¼ ë•Œ
    """
    if mode == "stream":
        return _listen_and_transcribe()
    
    elif mode == "file":
        if wavefile is None:
            raise ValueError("mode='file'ì¼ ë•ŒëŠ” wavefileì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        try:
            # OpenAI Whisper APIë¡œ ì§ì ‘ ì „ì‚¬
            with open(wavefile, "rb") as audio_file:
                response = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="ko"
                )
            
            return response.text
        
        except Exception as e:
            print(f"íŒŒì¼ ì „ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""
    
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” mode: {mode}. 'stream' ë˜ëŠ” 'file'ì„ ì‚¬ìš©í•˜ì„¸ìš”.")


if __name__ == '__main__':
    # CLI ëª¨ë“œ: ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹
    text = audio2text(mode="stream", wavefile="temp_audio.wav")
    print(f"\nìµœì¢… ê²°ê³¼: {text}")